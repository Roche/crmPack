---
title: "How to use parallel computing with extensions to the package, an example"
author: "Oliver Boix"
date: "14 March 2023"
output: rmarkdown::html_vignette
bibliography: vignettes.bib  
vignette: >
  %\VignetteIndexEntry{Parallel computing with extensions}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteDepends{crmPack}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction

One of the big advantages of `crmPack` over existing R implementations is its flexible framework based on the S4 classes and methods system [@Chambers2008]. Users can extend the existing functionality easily to the specific needs of the study [@Bove2019].

User defined extensions of classes and methods can be easily created and used together with existing `crmPack` classes and methods when setting up the study and performing analysis with observed data. As long as no parallel computing is used, no special care needs to be taken utilizing both native and user defined classes and methods.
When trial simulations need to be performed to derive the operational characteristics of a study setup, run times may become long with single core processing. The run time depends mainly on the number of study replications and MCMC samples. In cases of long run times, utilizing parallel computing, i.e. using multiple CPU cores, can overcome these challenge and decrease run times significantly.

Parallel computing is supported by `crmPack` by design, i.e. whenever the simulation call is invoked with parameters `parallel = TRUE` and `nCores =` more than one CPU, each core is initialized with the `crmPack` package and the global environment.

Due to the nature of the S4 object system, user defined classes and method that were defined in the global environment cannot just made available at each core by initializing the core with the global environment. User defined classes and method needs to be executed on any core that is initialized to warrant full functionality of the S4 objects. 

To use user written extension with parallel computing and `crmPack`, the user written code needs to be embedded in a function that must have the fixed name `crmpack_extensions`. This function is executed at any core at the time of initialization of the core, so that the user written S4 classes and methods are available along with the `crmPack` package.

The following paragraphs provide an example of how user defined extensions can be used utilizing parallel computing within `crmPack`.

## Usage

User written code needs to be embedded into a function that must be named `crmpack_extensions`. The following code gives a high level example, how this looks like.

```{r high level showcase, eval=FALSE}
crmpack_extensions <- function() {
  # ..... user code .....
}
```

When the simulation function is executed with the parameter `parallel = TRUE` and `nCores =` a number greater than one, `crmPack` evaluates if a function with the name `crmpack_extensions` exists in the global environment. In case this is true, the `crmpack_extensions` function is executed at the time of initialization of any core, i.e. the code within the function is available for the use with parallel computing.

Note that the `<<-` operator must be used for any new user defined constructor function that is defined within the wrapper function `crmpack_extensions`. The `<<-` operator will force the a function to become visible within the global environment.

It is expected as a good practice, that any newly newly written extension is first tested and validated with one CPU core, before utilizing the function `crmpack_extensions`. It is important to understand that any content of the function `crmpack_extensions` is executed directly at each core and will not be checked for validity or correctness. In case that the code run into errors, the returned error message while using parallel computing may be misleading and may not help to identify the root cause of problem. Debugging of newly written code is much easier without using parallel computing.

## Worked out example

Let us assume we want to utilize a two-parameter logistic regression model, where the slope parameter should have only positive values, with a normal prior that is truncated for the slope parameter, so that the slope can have only positive values. In this case we want to set the prior distribution of the slope parameter to be a truncated normal distribution bounded by zero on the lower end. This model representation is not part of the `crmPack` but can easily be added as shown below.
Please note that for this example the extra code is directly embedded into the `crmpack_extensions` function. As mentioned before, user should write and test their code first without embedding it into the `crmpack_extensions` function, to assure that no errors occurs during the whole execution and the results are as expected.

```{r LogisticNormalTruncPrior Definition, eval=FALSE}
library(crmPack)

crmpack_extensions <- function() {
  # LogisticNormalTruncPrior ----

  ## class ----

  #' `LogisticNormalTruncPrior`
  #'
  #' @description `r lifecycle::badge("experimental")`
  #'
  #' [`LogisticNormalTruncPrior`] is the class for the usual logistic regression
  #'  model with bivariate normal prior on the intercept and slope.
  #'
  #' @aliases LogisticNormalTruncPrior
  #' @export
  #'
  #' @slot mean1 the mean of the intercept
  #' @slot mean2 the mean of the slope
  #' @slot var1 the variance of the intercept
  #' @slot var2 the variance of the slope
  #'
  .LogisticNormalTruncPrior <- setClass(
    Class = "LogisticNormalTruncPrior",
    contains = "GeneralModel",
    slots = c(
      mean1 = "numeric",
      mean2 = "numeric",
      var1 = "numeric",
      var2 = "numeric"
    )
  )

  ## constructor ----

  #' @rdname LogisticNormalTruncPrior-class

  #' Initialization function for the `LogisticNormalTruncPrior` class
  #'
  #' @param mean1 the mean of the intercept
  #' @param mean2 the mean of the slope
  #' @param var1 the variance of the intercept
  #' @param var2 the variance of the slope
  #' @return the \code{\linkS4class{LogisticNormalTruncPrior}} object
  #'
  #' @export
  #' @keywords methods
  LogisticNormalTruncPrior <<- function(mean1, mean2, var1, var2) {
    .LogisticNormalTruncPrior(
      mean1 = mean1,
      mean2 = mean2,
      var1 = var1,
      var2 = var2,
      datamodel = function() {
        for (i in 1:nObs) {
          y[i] ~ dbern(mean[i])
          logit(mean[i]) <- alpha0 + alpha1 * x[i]
        }
      },
      priormodel = function() {
        alpha0 ~ dnorm(mean1, 1 / var1)
        alpha1 ~ dnorm(mean2, 1 / var2) %_% I(0, )
      },
      datanames = c("nObs", "y", "x"),
      modelspecs = function() {
        list(
          mean1 = mean1,
          mean2 = mean2,
          var1 = var1,
          var2 = var2
        )
      },
      init = function() {
        list(alpha0 = mean1, alpha1 = mean2)
      },
      sample = c("alpha0", "alpha1")
    )
  }

  ## dose ----

  #' @describeIn dose compute the dose level reaching a specific toxicity
  #'   probability.
  #'
  #' @aliases dose-LogisticNormalTruncPrior
  #' @export
  #'
  setMethod(
    f = "dose",
    signature = signature(
      x = "numeric",
      model = "LogisticNormalTruncPrior",
      samples = "Samples"
    ),
    definition = function(x, model, samples) {
      checkmate::assert_probabilities(x)
      checkmate::assert_subset(c("alpha0", "alpha1"), names(samples))
      assert_length(x, len = size(samples))

      alpha0 <- samples@data$alpha0
      alpha1 <- samples@data$alpha1
      (logit(x) - alpha0) / alpha1
    }
  )

  ## prob ----

  #' @describeIn prob compute the toxicity probability of a specific dose.
  #'
  #' @aliases prob-LogisticNormalTruncPrior
  #' @export
  #'
  setMethod(
    f = "prob",
    signature = signature(
      dose = "numeric",
      model = "LogisticNormalTruncPrior",
      samples = "Samples"
    ),
    definition = function(dose, model, samples) {
      checkmate::assert_numeric(dose, lower = 0L, any.missing = FALSE, min.len = 1)
      checkmate::assert_subset(c("alpha0", "alpha1"), names(samples))
      assert_length(dose, len = size(samples))

      alpha0 <- samples@data$alpha0
      alpha1 <- samples@data$alpha1
      1 / (1 + exp(-alpha0 - alpha1 * dose))
    }
  )
}
```

The newly created model `LogisticNormalTruncPrior` can now be used to set up a study. First, we load the function `crmpack_extensions` into the global environment, so that the function and corresponding classes and methods for `prob` and `dose` become available.

```{r initial study setup, eval=FALSE}
# Execute the user written extensions.
crmpack_extensions()

# Create the dose grid.
emptydata <- Data(
  doseGrid = c(
    10, 15, 20, 30, 40, 60, 80, 120, 160, 240, 320,
    480, 640, 960, 1280, 1920, 2400, 3000, 4000
  ),
  placebo = FALSE
)

# Create data for basic testing of the setup.
my_data <- Data(
  x = c(10, 20, 40, 80, 80, 160, 160),
  y = c(0, 0, 0, 0, 0, 1, 1),
  cohort = c(1, 2, 3, 4, 4, 5, 5),
  ID = 1:7,
  doseGrid = emptydata@doseGrid
)

# Setup the model.
my_model <- LogisticNormalTruncPrior(
  mean1 = -3,
  mean2 = 0.00075,
  var1 = 1,
  var2 = 0.000009
)

# Options used for simulations.
my_options <- McmcOptions(
  burnin = 100,
  step = 2,
  samples = 100,
  rng_kind = "Mersenne-Twister",
  rng_seed = 94
)

# Create mcmc samples.
my_samples <- mcmc(my_data, my_model, my_options)

# Plot the dose toxicity curve.
plot(my_samples, my_model, my_data)

# Specify increments.
my_increments <- IncrementsRelativeDLT(
  dlt_intervals = c(0, 1),
  increments = c(1, 0.5)
)

# Maximum dose.
this_max_dose <- maxDose(my_increments, my_data)

# Next best dose.
my_next_best <- NextBestMinDist(target = 0.3)
this_next_dose <- nextBest(my_next_best, this_max_dose, my_samples, my_model, my_data)$value

# Stopping rule.
my_stopping <- StoppingPatientsNearDose(nPatients = 9, percentage = 0)

# Stop trial based on criteria and observed data.
stopTrial(my_stopping, this_next_dose, my_samples, my_model, my_data)

# Cohorts size.
my_size <- CohortSizeDLT(
  dlt_intervals = c(0, 1),
  cohort_size = c(1, 3)
)

# Design.
my_design <- Design(
  model = my_model,
  nextBest = my_next_best,
  stopping = my_stopping,
  increments = my_increments,
  cohortSize = my_size,
  data = emptydata,
  startingDose = 10
)
```

After setting up the whole model, it is very useful to check the model decisions in case that no DLT is observed until a certain dose level before study simulations are performed. This check is also useful to additionally test the new written code.

```{r examine the design, eval=FALSE}
# Examine the design.
examine(my_design, my_options)
```

When examine runs as expected, study simulation can be performed. To demonstrate the difference between single core processing an multiple core processing, two scenarios where processed with single core processing.

```{r perform study simulations, eval=FALSE}
# Set up scenarios
safe_scenario <- probFunction(my_model, alpha0 = logit(0.05), alpha1 = (logit(0.3) - logit(0.05)) / 20000)
late_scenario <- probFunction(my_model, alpha0 = logit(0.05), alpha1 = (logit(0.3) - logit(0.05)) / 2000)
early_scenario <- probFunction(my_model, alpha0 = logit(0.05), alpha1 = (logit(0.3) - logit(0.05)) / 700)
toxic_scenario <- probFunction(my_model, alpha0 = logit(0.6), alpha1 = (logit(0.3) - logit(0.6)) / -300)
peak_scenario <- function(dose,
                          scenario = cbind(emptydata@doseGrid, c(rep(0.05, 11), rep(0.80, 8)))) {
  scenario[match(dose, scenario[, 1]), 2]
}


# Helper function that outputs the elapsed time.
report_time <- function(report_text) {
  cat(
    format(Sys.time(), usetz = TRUE),
    report_text,
    "done - elapsed time from start:",
    round(difftime(Sys.time(), start_time, units = "mins"), digits = 1),
    "\n"
  )
}

# Helper function that simulates a specific truth.
get_oc <- function(truth) {
  simulate(
    my_design,
    args = NULL,
    truth = truth,
    nsim = my_nsim,
    mcmcOptions = my_options,
    parallel = do_parallel,
    nCores = parallelly::availableCores()
  )
}

# get operation characteristics without utilizing parallel computing for selected truth
# (to reduce the run time).
time_no_parallel <- system.time({
  start_time <- Sys.time()
  cat(format(Sys.time(), usetz = TRUE), "start", "\n")

  my_nsim <- 100
  do_parallel <- FALSE

  safe <- get_oc(safe_scenario)

  report_time("safe (single core processing)")

  late <- get_oc(late_scenario)

  report_time("late (single core processing)")
})

# Get full operation characteristics utilizing parallel computing.
time <- system.time({
  start_time <- Sys.time()
  cat(format(Sys.time(), usetz = TRUE), "start", "\n")

  my_nsim <- 100
  do_parallel <- TRUE

  safe <- get_oc(safe_scenario)

  report_time("safe")

  late <- get_oc(late_scenario)

  report_time("late")

  early <- get_oc(early_scenario)

  report_time("early")

  toxic <- get_oc(toxic_scenario)

  report_time("toxic")

  peak <- get_oc(peak_scenario)

  report_time("peak")
})
```

